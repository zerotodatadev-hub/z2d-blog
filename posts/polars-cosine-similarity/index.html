<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Calculating cosine similarity between consecutive row embeddings in Polars (Python Tutorial) — starting naive, then making it pretty and fast with vectorized arrays.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Polars Arrays: Fast Cosine Similarity for Adjacent Embeddings | ZERO TO DATA</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Playfair+Display:700,900" rel="stylesheet">
<meta name="theme-color" content="#2563EB">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../rss.xml">
<link rel="canonical" href="https://z2d.io/posts/polars-cosine-similarity/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><link rel="stylesheet" type="text/css" href="../../assets/css/code.css" id="code-theme">
<script src="../../assets/js/copy.js" defer></script><script src="../../assets/js/code-toggle.js" defer></script><meta name="author" content="Max @ Z2D">
<link rel="prev" href="../hello-from-z2d/" title="Hello from Z2D" type="text/html">
<meta property="og:site_name" content="ZERO TO DATA">
<meta property="og:title" content="Polars Arrays: Fast Cosine Similarity for Adjacent Embeddings">
<meta property="og:url" content="https://z2d.io/posts/polars-cosine-similarity/">
<meta property="og:description" content="Calculating cosine similarity between consecutive row embeddings in Polars (Python Tutorial) — starting naive, then making it pretty and fast with vectorized arrays.">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-09-20T12:18:00Z">
<meta property="article:tag" content="beginner">
<meta property="article:tag" content="data-science">
<meta property="article:tag" content="embeddings">
<meta property="article:tag" content="intermediate">
<meta property="article:tag" content="performance">
<meta property="article:tag" content="polars">
<meta property="article:tag" content="python">
<meta property="article:tag" content="tutorial">
<meta property="article:tag" content="vectorization">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Header and menu bar -->
<div class="container">
      <header class="blog-header py-3"><div class="row nbb-header align-items-center">
          <div class="col-md-3 col-xs-2 col-sm-2" style="width: auto;">
            <button class="navbar-toggler navbar-light bg-light nbb-navbar-toggler" type="button" data-toggle="collapse" data-target=".bs-nav-collapsible" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse bs-nav-collapsible bootblog4-search-form-holder">
                
            </div>
        </div>
          <div class="col-md-6 col-xs-10 col-sm-10 bootblog4-brand" style="width: auto;">
            <a class="navbar-brand blog-header-logo text-dark" href="../../">

            <span id="blog-title">ZERO TO DATA</span>
        </a>
          </div>
            <div class="col-md-3 justify-content-end align-items-center bs-nav-collapsible collapse flex-collapse bootblog4-right-nav">
            <nav class="navbar navbar-light bg-white"><ul class="navbar-nav bootblog4-right-nav">
<li class="nav-item">
    <a href="index.md" id="sourcelink" class="nav-link">Source</a>
    </li>


                    
            </ul></nav>
</div>
    </div>
</header><nav class="navbar navbar-expand-md navbar-light bg-white static-top"><div class="collapse navbar-collapse bs-nav-collapsible" id="bs-navbar">
            <ul class="navbar-nav nav-fill d-flex w-100">
<li class="nav-item">
<a href="../../archive.html" class="nav-link">Archive</a>
                </li>
<li class="nav-item">
<a href="../../categories/" class="nav-link">Tags</a>
                </li>
<li class="nav-item">
<a href="../../rss.xml" class="nav-link">RSS feed</a>

                
            </li>
</ul>
</div>
<!-- /.navbar-collapse -->
</nav>
</div>

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        
        
        
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Polars Arrays: Fast Cosine Similarity for Adjacent Embeddings</a></h1>

        <div class="metadata">
            <p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">
                    Max @ Z2D
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2025-09-20T12:18:00Z" itemprop="datePublished" title="2025-09-20 12:18">2025-09-20 12:18</time></a>
            </p>
            
        <p class="sourceline"><a href="index.md" class="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <p>Text streams can shift quickly: a news feed may move from one story to the next, or a tweet stream may switch topics in seconds. Embedding models such as <a href="https://platform.openai.com/docs/guides/embeddings#use-cases">OpenAI's recent</a> <code>'text-embedding-3-large'</code> can convert the nuances of the text into meaningful vector representations. Comparing embeddings of adjacent entries helps flag these shifts — a drop in similarity is often the simplest signal of change.</p>
<blockquote>
<p><strong>TL;DR</strong>  </p>
<p>Cosine similarity between consecutive embeddings leveraging <a href="https://docs.pola.rs/user-guide/getting-started/">Polars</a>-native syntax (Python Tutorial).  </p>
<ul>
<li>Start with a simple, row-by-row <code>map_elements</code> approach (<em>make it work</em>).  </li>
<li>Switch to <code>pl.Array</code> and vectorized operations for big speedups (<em>make it fast</em>).  </li>
<li>In benchmarks, the Polars-native method runs ~17× faster than the naive Python version.  </li>
</ul>
</blockquote>
<h3>Exemplary dataset</h3>
<p>The embedding process converts text into vectors, which we can store in a single Polars column. Depending on the model, vector size can range from 100 to several thousand values.</p>
<blockquote>
<p><strong>Setup</strong>  </p>
<p>Code tested with:  </p>
<ul>
<li><em>Polars 1.33.1</em></li>
<li><em>NumPy 2.3.3</em></li>
</ul>
<p>Later versions may have small API differences.</p>
</blockquote>
<p>Let's look at a simpler <code>pl.DataFrame</code> where each embedding is a list with 3 values. After the embedding process, the object will look something like:</p>
<div class="code"><pre class="code literal-block"><span class="kn">import</span><span class="w"> </span><span class="nn">polars</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pl</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">"id"</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
        <span class="s2">"embedding"</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
            <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
            <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
        <span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>

<p>By default, Polars casts the data type of the <code>'embedding'</code> column to a list of floats (<code>pl.List[pl.Float64]</code>). We can inspect the types after printing the DataFrame.</p>
<div class="code"><pre class="code literal-block"><span class="go">shape: (3, 2)</span>
<span class="go">┌─────┬─────────────────┐</span>
<span class="go">│ id  ┆ embedding       │</span>
<span class="go">│ --- ┆ ---             │</span>
<span class="go">│ i64 ┆ list[f64]       │</span>
<span class="go">╞═════╪═════════════════╡</span>
<span class="go">│ 1   ┆ [1.0, 0.0, 0.0] │</span>
<span class="go">│ 2   ┆ [0.5, 0.5, 0.0] │</span>
<span class="go">│ 3   ┆ [0.0, 1.0, 0.0] │</span>
<span class="go">└─────┴─────────────────┘</span>
</pre></div>

<p>The vector representation encodes semantics and enables mathematical analysis. The <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a>, for example, is commonly used to measure similarity between vectors:</p>
<p>$$
\text{cosine_similarity}(\mathbf{a}, \mathbf{b}) =
\frac{\mathbf{a} \cdot \mathbf{b}}{|\mathbf{a}| \,|\mathbf{b}|} =
\frac{\sum_{i=1}^n a_i b_i}{\sqrt{\sum_{i=1}^n a_i^2} \sqrt{\sum_{i=1}^n b_i^2}}
$$</p>
<p>Applied to our use case, embeddings of two semantically similar texts will likely score higher than those of distinct texts. By calculating the cosine similarities for all adjacent embeddings (1-&gt;2, 2-&gt;3, ...) we can easily spot sudden drops of the scores. These drops pinpoint semantic shifts, detectable with change-point methods.</p>
<p>In the following, we work on the adjacent cosine similarity calculation following the  <strong>make it work -&gt; make it pretty -&gt; make it fast</strong> principle. Where we focus on providing a proof of concept before diving into optimizations using Polars-native syntax:</p>
<ol>
<li>naive and simple: calculate the cosine similarity row-by-row using <code>map_elements</code> by applying a custom function.</li>
<li>optimized and vectorized: array‑based approach leveraging polars native syntax</li>
</ol>
<h3>Naive First: Make It Work</h3>
<blockquote>
<p><strong>When to use this?</strong></p>
<p>There can be several situations in which <code>map_elements</code> and a custom function is useful</p>
<ul>
<li>It is a great start because it is simple and readable. Using a custom function will work even when the logic is not writable in pure Polars syntax.</li>
<li>The custom function is already available and you can directly use it</li>
<li>Performance is not a bottleneck and readability/simplicity counts</li>
</ul>
</blockquote>
<p>Looking again at the data structure, we see that we need to get access to the embedding vectors from two rows at the same time. For the first similarity calculation for example embeddings from row 1 and row 2:</p>
<div class="code"><pre class="code literal-block"><span class="go">shape: (3, 2)</span>
<span class="go">┌─────┬─────────────────┐</span>
<span class="go">│ id  ┆ embedding       │</span>
<span class="go">│ --- ┆ ---             │</span>
<span class="go">│ i64 ┆ list[f64]       │</span>
<span class="go">╞═════╪═════════════════╡</span>
<span class="go">│ 1   ┆ [1.0, 0.0, 0.0] │</span>
<span class="go">│ 2   ┆ [0.5, 0.5, 0.0] │</span>
<span class="go">│ 3   ┆ [0.0, 1.0, 0.0] │</span>
<span class="go">└─────┴─────────────────┘</span>
</pre></div>

<p>The easiest way for us is to create a second column <code>'_next'</code> that contains the embedding of the following row. <code>.shift(-1)</code> moves the embedding up by one row, and <code>.with_columns([...])</code> adds all the columns declared inside. Giving an <code>alias(...)</code> prevents shadowing of the existing <code>'embedding'</code> column in this case.</p>
<div class="code"><pre class="code literal-block"><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">with_columns</span><span class="p">([</span><span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">"embedding"</span><span class="p">)</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">"_next"</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>

<p>This trick allows us to conveniently access both embeddings within a single row. Further note that the last row, receives a <code>null</code> entry due to a missing successor.</p>
<div class="code"><pre class="code literal-block"><span class="go">shape: (3, 3)</span>
<span class="go">┌─────┬─────────────────┬─────────────────┐</span>
<span class="go">│ id  ┆ embedding       ┆ _next           │</span>
<span class="go">│ --- ┆ ---             ┆ ---             │</span>
<span class="go">│ i64 ┆ list[f64]       ┆ list[f64]       │</span>
<span class="go">╞═════╪═════════════════╪═════════════════╡</span>
<span class="go">│ 1   ┆ [1.0, 0.0, 0.0] ┆ [0.5, 0.5, 0.0] │</span>
<span class="go">│ 2   ┆ [0.5, 0.5, 0.0] ┆ [0.0, 1.0, 0.0] │</span>
<span class="go">│ 3   ┆ [0.0, 1.0, 0.0] ┆ null            │</span>
<span class="go">└─────┴─────────────────┴─────────────────┘</span>
</pre></div>

<p>Now we can iterate over individual rows using <code>map_elements()</code> without accessing multiple rows at once. We use the two vectors to call the custom similarity <code>_cosine_py(vec1, vec2)</code> function, which is defined later.</p>
<div class="code"><pre class="code literal-block"><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">with_columns</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="c1"># select required columns as struct. Allows dict-like usage in lambda</span>
        <span class="n">pl</span><span class="o">.</span><span class="n">struct</span><span class="p">((</span><span class="s2">"embedding"</span><span class="p">,</span> <span class="s2">"_next"</span><span class="p">))</span>
        <span class="o">.</span><span class="n">map_elements</span><span class="p">(</span>  <span class="c1"># iteration over entries (comparable to pandas .apply() )</span>
            <span class="c1"># parameterize the function for calculation of the cosine similarity</span>
            <span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">_cosine_py</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="s2">"embedding"</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="s2">"_next"</span><span class="p">]),</span>
            <span class="n">return_dtype</span><span class="o">=</span><span class="n">pl</span><span class="o">.</span><span class="n">Float32</span><span class="p">,</span>  <span class="c1"># specify the return</span>
            <span class="n">returns_scalar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">"cosine_similarity"</span><span class="p">)</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>

<p><code>map_elements()</code> expects a function that is called for every row. We need to take care of the function arguments. Selecting both vector columns, using <code>pl.struct()</code> is useful to for explicit and dict-like retrieval of the vectors. We simply wrap it inside a <code>lambda</code> to parameterize the cosine function call.</p>
<p>With the steps above in mind, we can compose the <code>cosine_adjacent_naive(df)</code> function that orchestrates the similarity calculation based on the DataFrame.</p>
<div class="code"><pre class="code literal-block"><span class="kn">import</span><span class="w"> </span><span class="nn">polars</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pl</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">_cosine_py</span><span class="p">(</span><span class="n">vec_1</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span> <span class="n">vec_2</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># handle cases where a vector is missing (eg. last entry because of shift)</span>
    <span class="k">if</span> <span class="n">vec_1</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">vec_2</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="c1"># cosine similarity calculation with numpy</span>
    <span class="n">v1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vec_1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">v2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vec_2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">n1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v1</span><span class="p">)</span>
    <span class="n">n2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v2</span><span class="p">)</span>

    <span class="c1"># escape division error</span>
    <span class="k">if</span> <span class="n">n1</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">n2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n1</span> <span class="o">*</span> <span class="n">n2</span><span class="p">))</span>


<span class="k">def</span><span class="w"> </span><span class="nf">cosine_adjacent_naive</span><span class="p">(</span><span class="n">df</span><span class="p">:</span> <span class="n">pl</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pl</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
    <span class="c1"># shift the embedding before calculation of the vector similarity</span>
    <span class="c1"># with the next row element</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">with_columns</span><span class="p">([</span><span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">"embedding"</span><span class="p">)</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">"_next"</span><span class="p">)])</span>

    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">with_columns</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="c1"># select required columns as struct. Allows dict-like usage in lambda</span>
            <span class="n">pl</span><span class="o">.</span><span class="n">struct</span><span class="p">((</span><span class="s2">"embedding"</span><span class="p">,</span> <span class="s2">"_next"</span><span class="p">))</span>
            <span class="o">.</span><span class="n">map_elements</span><span class="p">(</span>  <span class="c1"># iteration over entries (comparable to pandas .apply() )</span>
                <span class="c1"># parameterize the function for calculation of the cosine similarity</span>
                <span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">_cosine_py</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="s2">"embedding"</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="s2">"_next"</span><span class="p">]),</span>
                <span class="n">return_dtype</span><span class="o">=</span><span class="n">pl</span><span class="o">.</span><span class="n">Float32</span><span class="p">,</span>  <span class="c1"># specify the return</span>
                <span class="n">returns_scalar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">"cosine_similarity"</span><span class="p">)</span>
        <span class="p">]</span>
    <span class="c1"># cleanup of temporary series </span>
    <span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">"_next"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span>
</pre></div>

<p>Let's construct an example to prove our concept of the change point detection with following DataFrame.</p>
<div class="code"><pre class="code literal-block"><span class="n">df</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">"id"</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
        <span class="s2">"embedding"</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
            <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
            <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
            <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>  <span class="c1"># NOTE change</span>
            <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">],</span>
            <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">],</span>
        <span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="n">naive</span> <span class="o">=</span> <span class="n">cosine_adjacent_naive</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">naive</span><span class="o">.</span><span class="n">select</span><span class="p">([</span><span class="s2">"id"</span><span class="p">,</span> <span class="s2">"cosine_similarity"</span><span class="p">]))</span>
</pre></div>

<p>Inspecting <code>'cosine_similarity'</code>, the drop from 0.97 to 0.21 at row 3 stands out. It was calculated for the pair 3-&gt;4. We can detect it visually already and could further think of algorithms to programmatically mark areas of interest.</p>
<p>The <code>null</code> in the last output row is due to the missing successor and doesn't need to concern us.</p>
<div class="code"><pre class="code literal-block"><span class="go">shape: (6, 2)</span>
<span class="go">┌─────┬───────────────────┐</span>
<span class="go">│ id  ┆ cosine_similarity │</span>
<span class="go">│ --- ┆ ---               │</span>
<span class="go">│ i64 ┆ f32               │</span>
<span class="go">╞═════╪═══════════════════╡</span>
<span class="go">│ 1   ┆ 0.972511          │</span>
<span class="go">│ 2   ┆ 0.969231          │</span>
<span class="go">│ 3   ┆ 0.217524          │</span>
<span class="go">│ 4   ┆ 0.991241          │</span>
<span class="go">│ 5   ┆ 0.990148          │</span>
<span class="go">│ 6   ┆ null              │</span>
<span class="go">└─────┴───────────────────┘</span>
</pre></div>

<p>Great! We proved that the concept works and have written an algorithm to calculate the similarity scores for us. We know the pros and cons: the logic is readable and maintainable, but slow due to Python iteration.</p>
<p>After asking a few questions:</p>
<ul>
<li>Is the code fast enough for my purposes already?</li>
<li>How often do I need to run the processing?</li>
<li>Can I rewrite the logic optimized in Polars before the deadline?</li>
</ul>
<p>We can either finish up our solution, or we can move ahead to optimize the runtime.</p>
<h3>Make It Pretty and Fast: Vectorized Arrays</h3>
<blockquote>
<p><strong>When to use this?</strong></p>
<p>The vector calculation is based on the data type <code>pl.Array</code> and the <code>expr.arr</code> attribute.</p>
<ul>
<li>The <a href="https://docs.pola.rs/user-guide/expressions/lists-and-arrays/#the-data-type-array">series must be convertible to <code>pl.Array</code></a>. Requires homogeneous type and uniform length in all fields.</li>
<li>Performance and/or memory consumption counts</li>
</ul>
</blockquote>
<p>To unlock the performance and memory potential of Polars, we need to take care of the embedding column's data type. As stated initially the list of floats is cast to <code>pl.List[pl.Float64]</code> by default. Unfortunately Polars doesn't infer the <code>pl.Array</code> type automatically, which can be used for homogeneous types of fixed-size containers. And this is exactly what applies to our embeddings. Since all vectors were generated by the same model and processing, the embeddings qualify perfectly as arrays.</p>
<p>We can cast from list to array using the expression <code>expr.list.to_array(len)</code>. Therefore we extract the length of a single embedding vector. And convert the data type to array.</p>
<p>Before the implementation, let’s understand the required <a href="https://docs.pola.rs/api/python/stable/reference/expressions/array.html">array expressions</a> using another similar DataFrame.</p>
<div class="code"><pre class="code literal-block"><span class="n">mf</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">"id"</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
        <span class="s2">"embedding"</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span>
            <span class="p">[</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">],</span>
        <span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="c1"># retrieve the length of the first vector for conversion</span>
<span class="n">embedding_length</span> <span class="o">=</span> <span class="n">mf</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">"embedding"</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="o">.</span><span class="n">len</span><span class="p">()</span>

<span class="c1"># cast the list to array type</span>
<span class="n">mf</span> <span class="o">=</span> <span class="n">mf</span><span class="o">.</span><span class="n">with_columns</span><span class="p">([</span><span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">"embedding"</span><span class="p">)</span><span class="o">.</span><span class="n">list</span><span class="o">.</span><span class="n">to_array</span><span class="p">(</span><span class="n">embedding_length</span><span class="p">)])</span>

<span class="c1"># explore linear algebra using pl.Array</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">mf</span><span class="o">.</span><span class="n">with_columns</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="c1"># element-wise multiplication v1^2 , v2^2 , v3^2</span>
        <span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">"embedding"</span><span class="p">)</span> <span class="o">*</span> <span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">"embedding"</span><span class="p">))</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">"element_wise"</span><span class="p">),</span>

        <span class="c1"># same as above + summing up the vector's elements -&gt; dot product</span>
        <span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">"embedding"</span><span class="p">)</span> <span class="o">*</span> <span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">"embedding"</span><span class="p">))</span><span class="o">.</span><span class="n">arr</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">"dot"</span><span class="p">),</span>

    <span class="p">]</span>
<span class="p">)</span><span class="o">.</span><span class="n">with_columns</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="c1"># the second with_columns allows to reference precomputed and aliased columns</span>
        <span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s1">'dot'</span><span class="p">)</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">"norm"</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
</pre></div>

<p>The resulting DataFrame shows how to perform common vector calculations (such as element-wise multiplications) or array aggregations (such as the dot product).</p>
<div class="code"><pre class="code literal-block"><span class="go">shape: (2, 5)</span>
<span class="go">┌─────┬────────────────────┬───────────────────────┬───────┬───────────┐</span>
<span class="go">│ id  ┆ embedding          ┆ element_wise          ┆ dot   ┆ norm      │</span>
<span class="go">│ --- ┆ ---                ┆ ---                   ┆ ---   ┆ ---       │</span>
<span class="go">│ i64 ┆ array[f64, 3]      ┆ array[f64, 3]         ┆ f64   ┆ f64       │</span>
<span class="go">╞═════╪════════════════════╪═══════════════════════╪═══════╪═══════════╡</span>
<span class="go">│ 1   ┆ [1.0, 2.0, 3.0]    ┆ [1.0, 4.0, 9.0]       ┆ 14.0  ┆ 3.741657  │</span>
<span class="go">│ 2   ┆ [10.0, 10.0, 10.0] ┆ [100.0, 100.0, 100.0] ┆ 300.0 ┆ 17.320508 │</span>
<span class="go">└─────┴────────────────────┴───────────────────────┴───────┴───────────┘</span>
</pre></div>

<p>With the array syntax in mind, we can compose a clean polars-native expression to calculate the similarity scores. Let us look at the function <code>cosine_adjacent_vectorized(df)</code>. It is composed of multiple steps, where each step has a distinct purpose inside the corresponding <code>with_columns(...)</code> block:</p>
<ol>
<li>row shift making both vectors available</li>
<li>unlocking array potential via a cast <code>expr.list.to_array()</code>
</li>
<li>precomputation of formula variables using linear algebra through <code>expr.arr</code> syntax</li>
<li>cosine similarity calculation</li>
</ol>
<div class="code"><pre class="code literal-block"><span class="kn">import</span><span class="w"> </span><span class="nn">polars</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pl</span>

<span class="k">def</span><span class="w"> </span><span class="nf">cosine_adjacent_vectorized</span><span class="p">(</span>
    <span class="n">df</span><span class="p">:</span> <span class="n">pl</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">out_dtype</span><span class="p">:</span> <span class="n">pl</span><span class="o">.</span><span class="n">DataType</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Float32</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pl</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
    <span class="n">embedding_length</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">"embedding"</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="o">.</span><span class="n">len</span><span class="p">()</span>

    <span class="n">similarity</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">df</span><span class="o">.</span><span class="n">with_columns</span><span class="p">(</span>
            <span class="c1"># 1) row shift</span>
            <span class="c1"># shift the embedding before calculation of the similarity</span>
            <span class="c1"># with the next row element</span>
            <span class="c1"># columns can be accessed in the next with_columns block</span>
            <span class="p">[</span>
                <span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">"embedding"</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">"_current_em"</span><span class="p">),</span>
                <span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">"embedding"</span><span class="p">)</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">"_next_em"</span><span class="p">),</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="o">.</span><span class="n">with_columns</span><span class="p">(</span>
            <span class="c1"># 2) unlocking array potential via a cast `expr.list.to_array()`</span>
            <span class="c1"># convert both embedding columns to array this allows vectorized operations</span>
            <span class="c1"># in the the following with_columns section</span>
            <span class="p">[</span>
                <span class="c1"># convert current embedding</span>
                <span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">"_current_em"</span><span class="p">)</span><span class="o">.</span><span class="n">list</span><span class="o">.</span><span class="n">to_array</span><span class="p">(</span><span class="n">embedding_length</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">"_current_em"</span><span class="p">),</span>

                <span class="c1"># convert next embedding</span>
                <span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">"_next_em"</span><span class="p">)</span><span class="o">.</span><span class="n">list</span><span class="o">.</span><span class="n">to_array</span><span class="p">(</span><span class="n">embedding_length</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">"_next_em"</span><span class="p">),</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="o">.</span><span class="n">with_columns</span><span class="p">(</span>

            <span class="c1"># 3) precomputation of formula variables </span>
            <span class="p">[</span>
                <span class="c1"># # dot product</span>
                <span class="c1"># element-wise calculation</span>
                <span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">"_current_em"</span><span class="p">)</span> <span class="o">*</span> <span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">"_next_em"</span><span class="p">))</span>
                <span class="c1"># cast to array &amp; sum up the elements</span>
                <span class="o">.</span><span class="n">arr</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
                <span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">"_dot"</span><span class="p">),</span>

                <span class="c1"># #  current norm </span>
                <span class="c1"># element-wise calculation</span>
                <span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">"_current_em"</span><span class="p">)</span> <span class="o">*</span> <span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">"_current_em"</span><span class="p">))</span>
                <span class="o">.</span><span class="n">arr</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
                <span class="c1"># eventually compute the square root</span>
                <span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
                <span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">"_current_norm"</span><span class="p">),</span>

                <span class="c1"># # next norm</span>
                <span class="c1"># analog to above</span>
                <span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">"_next_em"</span><span class="p">)</span> <span class="o">*</span> <span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">"_next_em"</span><span class="p">))</span>
                <span class="o">.</span><span class="n">arr</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
                <span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
                <span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">"_next_norm"</span><span class="p">),</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="o">.</span><span class="n">with_columns</span><span class="p">(</span>

            <span class="c1"># 4) cosine similarity calculation</span>
            <span class="p">[</span>
                <span class="c1"># plug in the precomputed values</span>
                <span class="p">(</span> <span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">"_dot"</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">"_current_norm"</span><span class="p">)</span> <span class="o">*</span> <span class="n">pl</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="s2">"_next_norm"</span><span class="p">))</span> <span class="p">)</span>
                <span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">out_dtype</span><span class="p">)</span>
                <span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">"cosine_similarity"</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>
    <span class="c1"># cleanup of temporary series </span>
    <span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">"_next_em"</span><span class="p">,</span> <span class="s2">"_next_norm"</span><span class="p">,</span> <span class="s2">"_current_em"</span><span class="p">,</span> <span class="s2">"_current_norm"</span><span class="p">,</span> <span class="s2">"_dot"</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">similarity</span>
</pre></div>

<p>Now it's time to put the function into action and compare it to <code>cosine_adjacent_naive()</code>:</p>
<div class="code"><pre class="code literal-block"><span class="kn">import</span><span class="w"> </span><span class="nn">polars</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pl</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">"id"</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
        <span class="s2">"embedding"</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
            <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
            <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
            <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>  <span class="c1"># NOTE</span>
            <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">],</span>
            <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">],</span>
        <span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="n">opt</span> <span class="o">=</span> <span class="n">cosine_adjacent_vectorized</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">opt</span><span class="o">.</span><span class="n">select</span><span class="p">([</span><span class="s2">"id"</span><span class="p">,</span> <span class="s2">"cosine_similarity"</span><span class="p">]))</span>
</pre></div>

<p>Comparing the vectorized output block with the naive version above, we see identical similarity scores. And can conclude that both functions produce the same results.</p>
<div class="code"><pre class="code literal-block"><span class="go">shape: (6, 2)</span>
<span class="go">┌─────┬───────────────────┐</span>
<span class="go">│ id  ┆ cosine_similarity │</span>
<span class="go">│ --- ┆ ---               │</span>
<span class="go">│ i64 ┆ f32               │</span>
<span class="go">╞═════╪═══════════════════╡</span>
<span class="go">│ 1   ┆ 0.972511          │</span>
<span class="go">│ 2   ┆ 0.969231          │</span>
<span class="go">│ 3   ┆ 0.217524          │</span>
<span class="go">│ 4   ┆ 0.991241          │</span>
<span class="go">│ 5   ┆ 0.990148          │</span>
<span class="go">│ 6   ┆ null              │</span>
<span class="go">└─────┴───────────────────┘</span>
</pre></div>

<p>Great again - the Polars-native function works! It uses the efficient <code>pl.Array</code> data type and uses <code>expr.arr</code> to perform matrix calculations. So it should be faster, right?</p>
<p>Let’s benchmark whether shifting computation from Python to Polars’ Rust engine performs better.</p>
<h3>Benchmarking</h3>
<p>In a direct comparison, the <code>cosine_adjacent_vectorized(df)</code> clearly outperforms <code>cosine_adjacent_naive(df)</code>. It was benchmarked on my laptop computer with a dataset of 1,000,000 entries and embeddings of length 500, for a total of 3 runs.</p>
<div class="code"><pre class="code literal-block"><span class="go">Preparing DataFrame with 1,000,000 rows and embeddings of length 500.</span>
<span class="go">Data preparation took 8.98s</span>

<span class="go">Running benchmark (3 run(s) per method)</span>
<span class="go">vectorized | avg=2.81s best=2.04s worst=3.50s rows/s=355,696</span>
<span class="go">     naive | avg=48.11s best=46.84s worst=49.97s rows/s=20,785</span>
<span class="go">Benchmark complete.</span>
</pre></div>

<p>The Polars-native approach executes ~17 times faster on average. Depending on the size of the datasets you are processing, the gains pay off: 1 second vs 17 seconds, 1 minute vs 17 minutes, 1 hour vs 17 hours.</p>
<h3>Key Ideas to Reuse</h3>
<p>Beyond the speedup, this case shows general problem-solving strategies and highlights Polars’ array toolbox.</p>
<ul>
<li>
<strong>Make it work → make it pretty → make it fast</strong>: Create value early on with the approach you can realize the fastest. Improve if you need to.</li>
<li>
<strong>prefer vectorized over iterations</strong>: cast data to <code>pl.Array</code> for vector and matrix operations for homogeneous and fixed size data</li>
<li>Keep computations <strong>explicit and composable</strong>: it’s easier to reason about, debug, and optimize.</li>
</ul>
</div>
    <aside class="postpromonav"><nav><ul itemprop="keywords" class="tags">
<li><a class="tag p-category" href="../../categories/beginner/" rel="tag">beginner</a></li>
            <li><a class="tag p-category" href="../../categories/data-science/" rel="tag">data-science</a></li>
            <li><a class="tag p-category" href="../../categories/embeddings/" rel="tag">embeddings</a></li>
            <li><a class="tag p-category" href="../../categories/intermediate/" rel="tag">intermediate</a></li>
            <li><a class="tag p-category" href="../../categories/performance/" rel="tag">performance</a></li>
            <li><a class="tag p-category" href="../../categories/polars/" rel="tag">polars</a></li>
            <li><a class="tag p-category" href="../../categories/python/" rel="tag">python</a></li>
            <li><a class="tag p-category" href="../../categories/tutorial/" rel="tag">tutorial</a></li>
            <li><a class="tag p-category" href="../../categories/vectorization/" rel="tag">vectorization</a></li>
        </ul>
<ul class="pager hidden-print">
<li class="previous">
                <a href="../hello-from-z2d/" rel="prev" title="Hello from Z2D">Previous post</a>
            </li>
        </ul></nav></aside><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity="sha384-3lJUsx1TJHt7BA4udB5KPnDrlkO8T6J6v/op7ui0BbCjvZ9WqV4Xm6DTP6kQ/iBH" crossorigin="anonymous"></script><script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']]}});
        </script></article><!--End of body content--><footer id="footer">
            Contents © 2025         <a href="mailto:zerotodata.dev@gmail.com">Max @ Z2D</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
            
        </footer>
</div>
</div>


        <script src="../../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
    </script>
</body>
</html>
